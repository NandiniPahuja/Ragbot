#!/usr/bin/env python3
"""
upload_pdf.py: extract text from a PDF, split into chunks, embed, and add to the project's Chroma vectorstore.

Usage:
    python upload_pdf.py path/to/file.pdf
"""
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.documents import Document

import argparse
import os
from dotenv import load_dotenv


def main():
    parser = argparse.ArgumentParser(
        description="Upload a PDF into the existing Chroma vectorstore"
    )
    parser.add_argument(
        "pdf_path", help="Path to the PDF file to upload"
    )
    args = parser.parse_args()

    pdf_path = args.pdf_path
    if not os.path.isfile(pdf_path):
        print(f"❌ File not found: {pdf_path}")
        return

    # Load OpenAI API key from .env or environment
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("Error: OPENAI_API_KEY not set. Please set in environment or .env file.")
        return

    # Load PDF and extract pages
    loader = PyPDFLoader(pdf_path)
    raw_docs = loader.load()

    # Split into pure-text chunks for embedding
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = []
    for doc in raw_docs:
        chunks.extend(splitter.split_text(doc.page_content))

    # Prepare embeddings and load existing vectorstore
    embeddings = OpenAIEmbeddings(openai_api_key=api_key)
    vectordb = Chroma(
        persist_directory="vector_index",
        embedding_function=embeddings,
    )

    # Wrap each chunk in a LangChain Document so IDs are autogenerated
    docs_to_add = [Document(page_content=chunk) for chunk in chunks]
    vectordb.add_documents(docs_to_add)
    vectordb.persist()

    print(f"✅ Uploaded and indexed '{pdf_path}' into the Chroma vectorstore.")


if __name__ == "__main__":
    main()
